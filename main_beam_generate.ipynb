{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:29.730850Z",
     "iopub.status.busy": "2025-11-14T00:13:29.730592Z",
     "iopub.status.idle": "2025-11-14T00:13:30.049078Z",
     "shell.execute_reply": "2025-11-14T00:13:30.048268Z",
     "shell.execute_reply.started": "2025-11-14T00:13:29.730831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/300dim-utils/vocab_300dim.pkl\n",
      "/kaggle/input/300dim-utils/embedding_matrix_300dim.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:30.050399Z",
     "iopub.status.busy": "2025-11-14T00:13:30.049918Z",
     "iopub.status.idle": "2025-11-14T00:13:49.425633Z",
     "shell.execute_reply": "2025-11-14T00:13:49.425032Z",
     "shell.execute_reply.started": "2025-11-14T00:13:30.050373Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3440786f0742cf98dc72bd1c3f9ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c83a2bc9544d9ea5dc60e92bffaaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00004-2d5a1467fff108(‚Ä¶):   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203f362553a647b689e903621986d788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00004-5852b56a2bd28f(‚Ä¶):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268f546e9c6741b083360fafa9a69e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00004-a26307300439e9(‚Ä¶):   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e041684c557433a810f12f72cc4bee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00004-d243063613e5a0(‚Ä¶):   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f787b53beb44e70bf5f8a7134510200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001-869c898b5(‚Ä¶):   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136e7c8dbf1946e1b1be47bff9f82d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8593482bdeb0419c98c4422efdb4f1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:49.955278Z",
     "iopub.status.busy": "2025-11-14T00:13:49.955019Z",
     "iopub.status.idle": "2025-11-14T00:13:54.963551Z",
     "shell.execute_reply": "2025-11-14T00:13:54.962376Z",
     "shell.execute_reply.started": "2025-11-14T00:13:49.955250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ftfy\n",
      "Successfully installed ftfy-6.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:54.968949Z",
     "iopub.status.busy": "2025-11-14T00:13:54.967897Z",
     "iopub.status.idle": "2025-11-14T00:13:56.102940Z",
     "shell.execute_reply": "2025-11-14T00:13:56.102307Z",
     "shell.execute_reply.started": "2025-11-14T00:13:54.968908Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "\n",
    "def clean_and_normalize_text(text: str) -> str:\n",
    "    # Step 1: Use ftfy for general mojibake fixing\n",
    "    text = fix_text(text)\n",
    "\n",
    "    # Step 2: Replace known leftover mojibake manually\n",
    "    replacements = {\n",
    "        # Common double quotes (mojibake)\n",
    "        '√¢‚Ç¨≈ì': '\"',  # ‚Äú\n",
    "        '√¢‚Ç¨¬ù': '\"',  # ‚Äù (some datasets use this variant)\n",
    "        '√¢‚Ç¨': '\"',   # leftover quote fragments\n",
    "        '√Ç¬´': '\"',   # ¬´\n",
    "        '√Ç¬ª': '\"',   # ¬ª\n",
    "    \n",
    "        # Common single quotes / apostrophes\n",
    "        '√¢‚Ç¨Àú': \"'\",  # ‚Äò\n",
    "        '√¢‚Ç¨‚Ñ¢': \"'\",  # ‚Äô\n",
    "        '√¢‚Ç¨¬≤': \"'\",  # ‚Ä≤ (prime used instead of apostrophe sometimes)\n",
    "        '√¢‚Ç¨¬≥': '\"',  # ‚Ä≥ double prime ‚Üí double quote\n",
    "    \n",
    "        # En dash / em dash\n",
    "        '√¢‚Ç¨‚Äú': '-',  # ‚Äì\n",
    "        '√¢‚Ç¨‚Äù': '-',  # ‚Äî\n",
    "        '√¢ÀÜ‚Äô': '-',  # ‚àí (minus symbol)\n",
    "    \n",
    "        # Ellipsis\n",
    "        '√¢‚Ç¨¬¶': '...',  # ‚Ä¶\n",
    "    \n",
    "        # Currency symbols (guess replacements)\n",
    "        '√¢‚Äö¬¨': '‚Ç¨',  # euro\n",
    "        '√¢‚Äö¬£': '¬£',  # pound\n",
    "        '√¢‚Äö¬•': '¬•',  # yen\n",
    "    \n",
    "        # Bullets / middots\n",
    "        '√¢‚Ç¨¬¢': '‚Ä¢',  # bullet\n",
    "        '√Ç¬∑': '¬∑',   # middle dot\n",
    "    \n",
    "        # Accented letter fixes (when partial ftfy fails)\n",
    "        '√É¬©': '√©',\n",
    "        '√É¬®': '√®',\n",
    "        '√É¬¢': '√¢',\n",
    "        '√É¬¥': '√¥',\n",
    "        '√É√†': '√†',\n",
    "        '√É¬™': '√™',\n",
    "        '√É¬´': '√´',\n",
    "        '√É¬π': '√π',\n",
    "        '√É¬º': '√º',\n",
    "        '√É¬∂': '√∂',\n",
    "        '√É‚Äû': '√Ñ',\n",
    "        '√É≈ì': '√ú',\n",
    "        '√É‚Äì': '√ñ',\n",
    "        '√É≈∏': '√ü',\n",
    "        '√É¬±': '√±',\n",
    "    \n",
    "        # Occasionally seen garbage characters\n",
    "        '√Ç': '',     # stray non-breaking space marker\n",
    "        '√¢ÀÜ‚Ä†': '',   # delta artifact\n",
    "        '√¢ÀÜ≈æ': '‚àû',  # infinity sign\n",
    "        '√¢‚Äû¬¢': '‚Ñ¢',  # trademark\n",
    "        '√¢≈°¬´': '',   # stray symbol\n",
    "        '√¢≈ì‚Äù': '‚úî',  # checkmark\n",
    "    \n",
    "        # Daggers and unknown artifacts\n",
    "        '‚Ä†': ' ',\n",
    "        '‚Ä°': ' ',\n",
    "        'ÔøΩ': ''      # replacement char for unknown glyph\n",
    "    }\n",
    "\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "\n",
    "    # # Step 3: Normalize repeated/mismatched quotes (optional formatting cleanup)\n",
    "    # text = re.sub(r'\\s+\"', '\"', text)          # Remove extra leading spaces before quotes\n",
    "    # text = re.sub(r'\"\\s+', '\" ', text)         # Ensure a space after closing quotes when needed\n",
    "\n",
    "    # # Step 4: Compact multiple spaces\n",
    "    # text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # # Step 5: Strip leading/trailing spaces\n",
    "    # text = text.strip()\n",
    "\n",
    "    return text\n",
    "for i, item in enumerate(ds[\"train\"]):\n",
    "    if i == 529932:\n",
    "        print(\"j\")\n",
    "        text = clean_and_normalize_text(item['text'])  # Apply ftfy here\n",
    "        \n",
    "        if \"Once upon a time there was a small, humble dog named Mittens. She loved to go for walks in the woods,\" in text:\n",
    "            print(\"Found at index:\", i)\n",
    "            print(text)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:56.104489Z",
     "iopub.status.busy": "2025-11-14T00:13:56.103812Z",
     "iopub.status.idle": "2025-11-14T00:13:58.276720Z",
     "shell.execute_reply": "2025-11-14T00:13:58.275811Z",
     "shell.execute_reply.started": "2025-11-14T00:13:56.104463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:58.278012Z",
     "iopub.status.busy": "2025-11-14T00:13:58.277539Z",
     "iopub.status.idle": "2025-11-14T00:14:14.862696Z",
     "shell.execute_reply": "2025-11-14T00:14:14.861857Z",
     "shell.execute_reply.started": "2025-11-14T00:13:58.277986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model 'en_core_web_sm'...\n",
      "spaCy tokenizer ready.\n",
      "\n",
      "üöÄ Tokenization Begins (parallel mode with cleaning)...\n",
      "\n",
      "Processing 'train' split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2186870187b40369776c6554100b0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning & Tokenizing train (num_proc=4):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5056f034a5bc4a36ad36ce49b59f5bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved 'train' to 'tokenized_hf_dataset/train'.\n",
      "\n",
      "Processing 'validation' split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abe4551c204416e833c84e90043ad9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning & Tokenizing validation (num_proc=4):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c4964c5c684a6c8d98f0c074f2752e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished and saved 'validation' to 'tokenized_hf_dataset/validation'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading spaCy model 'en_core_web_sm'...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "tokenizer = nlp.tokenizer\n",
    "print(\"spaCy tokenizer ready.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes a single text string.\"\"\"\n",
    "    tokens = [t.text for t in tokenizer(str(text)) if not t.is_space]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_ds_path = \"./tokenized_hf_dataset\"\n",
    "final_combined_path = \"./combined_tokenized_texts_300dim_v2.pkl\"\n",
    "\n",
    "print(\"\\n Tokenization Begins ...\")\n",
    "\n",
    "def process_batch(batch):\n",
    "    # THE FIX: Apply the cleaning function before tokenizing\n",
    "    cleaned_texts = [clean_and_normalize_text(t) for t in batch[\"text\"]]\n",
    "    return {\"tokens\": [tokenize_text(t) for t in cleaned_texts]}\n",
    "\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    if split not in ds:\n",
    "        print(f\"‚ö†Ô∏è Split '{split}' not found in dataset ‚Äî skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing '{split}' split...\")\n",
    "    num_proc = min(4, os.cpu_count() or 1)\n",
    "\n",
    "    tokenized_split = ds[split].map(\n",
    "        process_batch,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=num_proc,\n",
    "        desc=f\"Cleaning & Tokenizing {split}\"\n",
    "    )\n",
    "    split_save_path = os.path.join(tokenized_ds_path, split)\n",
    "    tokenized_split.save_to_disk(split_save_path)\n",
    "    print(f\"Finished and saved '{split}' to '{split_save_path}'.\")\n",
    "\n",
    "# Commented section was used to make vocabulary from train and val both\n",
    "\n",
    "# if os.path.exists(final_combined_path):\n",
    "#     print(f\"\\n Found final combined file at '{final_combined_path}'. Nothing to do.\")\n",
    "# else:\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"\\n Streaming tokenized splits to a single .pkl file: '{final_combined_path}'...\")\n",
    "    \n",
    "#     total_lines = 0\n",
    "#     with open(final_combined_path, \"wb\") as f_out:\n",
    "#         for split in [\"train\", \"validation\"]:\n",
    "#             split_save_path = os.path.join(tokenized_ds_path, split)\n",
    "#             if not os.path.exists(split_save_path):\n",
    "#                 continue\n",
    "            \n",
    "#             print(f\"  Streaming from '{split}' split...\")\n",
    "#             tokenized_dataset = Dataset.load_from_disk(split_save_path)\n",
    "\n",
    "#             for example in tqdm(tokenized_dataset, desc=f\"  Pickling {split}\"):\n",
    "#                 tokens = example['tokens']\n",
    "#                 if tokens:\n",
    "#                     pickle.dump(tokens, f_out)\n",
    "#                     total_lines += 1\n",
    "\n",
    "#     # Save metadata for the vocab builder's progress bar\n",
    "#     # with open(metadata_path, \"w\") as f_meta:\n",
    "#     #     json.dump({\"total_lines\": total_lines}, f_meta)\n",
    "\n",
    "#     print(\"\\n Streaming complete.\")\n",
    "#     print(f\"Final combined file '{final_combined_path}' created with {total_lines:,} tokenized sentences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.200538Z",
     "iopub.status.busy": "2025-11-14T00:14:15.200380Z",
     "iopub.status.idle": "2025-11-14T00:14:15.219511Z",
     "shell.execute_reply": "2025-11-14T00:14:15.218954Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.200525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.220739Z",
     "iopub.status.busy": "2025-11-14T00:14:15.220516Z",
     "iopub.status.idle": "2025-11-14T00:14:15.242448Z",
     "shell.execute_reply": "2025-11-14T00:14:15.241863Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.220725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_ddp_v8.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_ddp_v8.py\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Config\n",
    "\n",
    "CONTEXT_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "HIDDEN_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 5\n",
    "DROPOUT = 0.1\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "# Dataset \n",
    "class ChunkedSequenceDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, hf_dataset, word2idx, context_len, sos_id, eos_id, unk_id):\n",
    "        self.word2idx = word2idx\n",
    "        self.context_len = context_len\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.unk_id = unk_id\n",
    "\n",
    "        print(f\"Preparing sequences (context_len={context_len})...\")\n",
    "        self.chunks = self._prepare_chunks(hf_dataset)\n",
    "        print(f\"Created {len(self.chunks):,} chunks total.\")\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        return [self.sos_id] + [self.word2idx.get(tok, self.unk_id) for tok in tokens] + [self.eos_id]\n",
    "\n",
    "    def _prepare_chunks(self, dataset):\n",
    "        all_chunks = []\n",
    "        for item in tqdm(dataset, desc=\"Converting to chunks\"):\n",
    "            tokens = item.get(\"tokens\", None)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            ids = self._tokens_to_ids(tokens)\n",
    "\n",
    "            # Chunk into (context_len + 1) for input/target shifting \n",
    "            for i in range(0, max(1, len(ids) - 1), self.context_len + 1):\n",
    "                chunk = ids[i : i + self.context_len + 1]  \n",
    "                if len(chunk) > 1:  \n",
    "                    all_chunks.append(chunk)\n",
    "        return all_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each chunk is already a list[int]\n",
    "        return torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Collate Function ‚Äî Handles padding + input/target split\n",
    "def collate_batch(batch, pad_id, context_len):\n",
    "    \n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        input_seq = seq[:-1][:context_len]   # up to context_len (leftmost tokens)\n",
    "        target_seq = seq[1:][:context_len]   # shifted by one done, will not do in training now\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "\n",
    "    padded_inputs = pad_sequence(input_seqs, batch_first=True, padding_value=pad_id)\n",
    "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "    if padded_inputs.size(1) < context_len:\n",
    "        pad_width = context_len - padded_inputs.size(1)\n",
    "        pad_tensor = torch.full((padded_inputs.size(0), pad_width), pad_id, dtype=torch.long)\n",
    "        padded_inputs = torch.cat([padded_inputs, pad_tensor], dim=1)\n",
    "        padded_targets = torch.cat([padded_targets, pad_tensor], dim=1)\n",
    "    elif padded_inputs.size(1) > context_len:\n",
    "        padded_inputs = padded_inputs[:, :context_len]\n",
    "        padded_targets = padded_targets[:, :context_len]\n",
    "\n",
    "    # Attention mask: 1 = real token, 0 = pad\n",
    "    attention_mask = (padded_inputs != pad_id).long()\n",
    "\n",
    "    return padded_inputs, padded_targets, attention_mask\n",
    "\n",
    "\n",
    "# Layers \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model):\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.from_numpy(pe)  # (max_len, d_model)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, output_attentions: bool = False):\n",
    "        # x: (b, t, d_model)\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        q = self.wq(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scores: (b, heads, t, t)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # masking + softmax in float32 to avoid fp16 overflow\n",
    "        orig_dtype = scores.dtype\n",
    "        scores = scores.float()\n",
    "\n",
    "        causal = torch.tril(torch.ones((t, t), dtype=torch.bool, device=x.device))\n",
    "        allowed = causal.unsqueeze(0).unsqueeze(0)  # (1,1,t,t)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            pad_bool = padding_mask if padding_mask.dtype == torch.bool else (padding_mask == 0)\n",
    "            key_is_real = (~pad_bool).unsqueeze(1).unsqueeze(2)  # (b,1,1,t)\n",
    "            allowed = allowed & key_is_real\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "        else:\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "\n",
    "        scores = scores.masked_fill(~allowed, torch.finfo(scores.dtype).min)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        attn = attn.to(v.dtype)\n",
    "        context = torch.matmul(attn, v)  # (b, heads, t, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(b, t, self.d_model)\n",
    "        out = self.wo(context)\n",
    "\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or (4 * d_model)\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.activation(self.fc1(x))))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, hidden_dim=4 * d_model, dropout=dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None,output_attentions=False):\n",
    "        # Self-attention (with pre-LN)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, attn_weights = self.self_attn(x_norm, padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm2 = self.ln2(x)\n",
    "        ff_out = self.ff(x_norm2)\n",
    "        x = x + self.dropout(ff_out)\n",
    "\n",
    "        return x,attn_weights\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, max_len, dropout=0.1, embedding_weights=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embedding.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "            if freeze_embeddings:\n",
    "                self.token_embedding.weight.requires_grad = False\n",
    "\n",
    "        pe = sinusoidal_positional_encoding(max_len, d_model)  # (max_len, d_model)\n",
    "        self.register_buffer(\"position_encoding\", pe)  # persistent buffer\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.final_ln = LayerNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,output_attentions=False):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        b, t = input_ids.size()\n",
    "        tok_emb = self.token_embedding(input_ids)  # (b, t, d_model)\n",
    "        pos_emb = self.position_encoding[:t, :].unsqueeze(0).expand(b, -1, -1)  # (b, t, d_model)\n",
    "        x = tok_emb + pos_emb  # (b, t, d_model)\n",
    "\n",
    "        # attention_mask: (b, t) with 1 for real tokens, 0 for pad\n",
    "        if attention_mask is None:\n",
    "            key_padding_mask = None\n",
    "        else:\n",
    "            \n",
    "            key_padding_mask = (attention_mask == 0)  # shape (b, t), dtype=bool\n",
    "\n",
    "\n",
    "        all_attentions = []\n",
    "  \n",
    "        for layer in self.layers:\n",
    "            if output_attentions:\n",
    "                x, layer_attn = layer(x, key_padding_mask=key_padding_mask, output_attentions=True)\n",
    "                all_attentions.append(layer_attn)  # list of tensors (b, heads, t, t)\n",
    "            else:\n",
    "                x,_ = layer(x, key_padding_mask=key_padding_mask)\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.output_linear(x)\n",
    "        return logits, all_attentions\n",
    "\n",
    "\n",
    "\n",
    "# Training / Main\n",
    "def main():\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Device:', DEVICE)\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "\n",
    "    tokenized_ds_path = \"tokenized_hf_dataset\"\n",
    "    VOCAB_SAVE_PATH = \"/kaggle/input/300dim-utils/vocab_300dim.pkl\"\n",
    "    EMBEDDING_MATRIX_SAVE_PATH = \"/kaggle/input/300dim-utils/embedding_matrix_300dim.pkl\"\n",
    "\n",
    "    # Load vocab\n",
    "    print(f\"\\n Found saved vocabulary file at '{VOCAB_SAVE_PATH}'. Loading...\")\n",
    "    with open(VOCAB_SAVE_PATH, \"rb\") as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "        word2idx = vocab_data['word2idx']\n",
    "        idx2word = vocab_data['idx2word']\n",
    "    print(f\"Loaded vocabulary with {len(word2idx):,} tokens.\")\n",
    "    VOCAB_SIZE = len(word2idx)\n",
    "\n",
    "    # Load embedding matrix\n",
    "    EMBEDDING_DIM = 100\n",
    "    print(f\"\\n Found saved embedding matrix file at '{EMBEDDING_MATRIX_SAVE_PATH}'. Loading...\")\n",
    "    with open(EMBEDDING_MATRIX_SAVE_PATH, \"rb\") as f:\n",
    "        embedding_matrix = pickle.load(f)\n",
    "    print(f\"Loaded embedding matrix with shape {embedding_matrix.shape}.\")\n",
    "\n",
    "    print(\" Loading tokenized dataset...\")\n",
    "    from datasets import load_from_disk\n",
    "    train_ds = load_from_disk(os.path.join(tokenized_ds_path, \"train\"))\n",
    "    val_ds = load_from_disk(os.path.join(tokenized_ds_path, \"validation\"))\n",
    "\n",
    "    # Build PyTorch Datasets\n",
    "    print(\"Building Dataset\")\n",
    "    train_dataset = ChunkedSequenceDataset(train_ds, word2idx, CONTEXT_LEN, SOS_ID, EOS_ID, UNK_ID)\n",
    "    val_dataset = ChunkedSequenceDataset(val_ds, word2idx, CONTEXT_LEN, SOS_ID, EOS_ID, UNK_ID)\n",
    "\n",
    "    # DataLoaders\n",
    "    collate_fn = partial(collate_batch, pad_id=PAD_ID, context_len=CONTEXT_LEN)\n",
    "\n",
    "    print(\"Building Sampler\")\n",
    "    train_sampler = DistributedSampler(train_dataset, shuffle=True)\n",
    "    val_sampler = DistributedSampler(val_dataset, shuffle=False)\n",
    "\n",
    "    print(\"Building Loader\")\n",
    "    SAVE_DIR = \"./checkpoints\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    USE_WANDB = True\n",
    "\n",
    "    # Init wandb only on rank 0\n",
    "    rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"  s\n",
    "    \n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.require(\"service\")\n",
    "        wandb.init(\n",
    "            project=\"transformer-training\",\n",
    "            mode=\"offline\",   \n",
    "            config={\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"lr\": LR,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"hidden_dim\": HIDDEN_DIM,\n",
    "                \"num_layers\": NUM_LAYERS,\n",
    "                \"num_heads\": NUM_HEADS\n",
    "            }\n",
    "        )\n",
    "   \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              sampler=train_sampler, collate_fn=collate_fn,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            sampler=val_sampler, collate_fn=collate_fn,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    print(\"Starting Model\")\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        max_len=CONTEXT_LEN,\n",
    "        dropout=DROPOUT,\n",
    "        embedding_weights=embedding_matrix,\n",
    "        freeze_embeddings=True\n",
    "    ).to(device)\n",
    "\n",
    "    model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    pad_idx = PAD_ID\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    train_losses, val_losses, perplexities = [], [], []\n",
    "    \n",
    "    # =============== TRAINING LOOP ===============\n",
    "    print(\"Training Begins....\")\n",
    "    # Path to checkpoint\n",
    "    CKPT_PATH = \"./epoch_10.pt\"  # example: resume from epoch 5\n",
    "    START_EPOCH = 1  # default if no checkpoint\n",
    "    \n",
    "    if os.path.exists(CKPT_PATH):\n",
    "        print(f\"Loading checkpoint from {CKPT_PATH} ...\")\n",
    "        checkpoint = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "    \n",
    "        # Restore model parameters\n",
    "        model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "        train_losses = checkpoint.get(\"train_losses\", [])\n",
    "        val_losses = checkpoint.get(\"val_losses\", [])\n",
    "        perplexities = checkpoint.get(\"perplexities\", [])\n",
    "    \n",
    "        # Continue from  epoch\n",
    "        START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    \n",
    "        print(f\" Resumed from epoch {checkpoint['epoch']} ‚Äî continuing from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\" No checkpoint found ‚Äî starting training from scratch\")\n",
    "\n",
    "    \n",
    "    for epoch in range(START_EPOCH, EPOCHS + 1):\n",
    "        torch.cuda.reset_peak_memory_stats(DEVICE)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_steps = 0\n",
    "    \n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch} train\")\n",
    "    \n",
    "        for step, (input_ids, target_ids, _) in pbar:\n",
    "            input_ids = input_ids.to(DEVICE, non_blocking=True)\n",
    "            target_ids = target_ids.to(DEVICE, non_blocking=True)\n",
    "            attention_mask = (input_ids != pad_idx).long()\n",
    "    \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits,_ = model(input_ids, attention_mask=attention_mask)\n",
    "                logits_flat = logits.view(-1, VOCAB_SIZE)\n",
    "                targets_flat = target_ids.view(-1)\n",
    "                loss = criterion(logits_flat, targets_flat)\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            avg_loss = running_loss / total_steps\n",
    "            pbar.set_postfix({'train_loss': avg_loss})\n",
    "    \n",
    "        train_loss = running_loss / total_steps\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        # =============== VALIDATION ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, target_ids, _ in tqdm(val_loader, desc=f\"Epoch {epoch} val\"):\n",
    "                input_ids = input_ids.to(DEVICE, non_blocking=True)\n",
    "                target_ids = target_ids.to(DEVICE, non_blocking=True)\n",
    "                attention_mask = (input_ids != pad_idx).long()\n",
    "    \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits,all_attns = model(input_ids, attention_mask=attention_mask, output_attentions = True)\n",
    "                    logits_flat = logits.view(-1, VOCAB_SIZE)\n",
    "                    targets_flat = target_ids.view(-1)\n",
    "                    loss = criterion(logits_flat, targets_flat)\n",
    "    \n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "    \n",
    "        val_loss /= val_steps\n",
    "        val_losses.append(val_loss)\n",
    "        perplexity = math.exp(val_loss)\n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "        peak_mem = torch.cuda.max_memory_allocated(DEVICE) / (1024 ** 3)\n",
    "    \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, PPL={perplexity:.2f}, PeakMem={peak_mem:.2f} GB\")\n",
    "    \n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"perplexity\": perplexity,\n",
    "                \"peak_gpu_memory_gb\": peak_mem\n",
    "            })\n",
    "    \n",
    "        # =============== CHECKPOINTING ===============\n",
    "        ckpt_path = os.path.join(SAVE_DIR, f\"epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.module.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"perplexities\": perplexities\n",
    "        }, ckpt_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"loss_curve.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), perplexities, label=\"Perplexity\", color=\"purple\", marker='o')\n",
    "    plt.title(\"Validation Perplexity Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"perplexity_curve.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.finish()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.243377Z",
     "iopub.status.busy": "2025-11-14T00:14:15.243027Z",
     "iopub.status.idle": "2025-11-14T00:30:23.131840Z",
     "shell.execute_reply": "2025-11-14T00:30:23.131084Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.243358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 train_ddp_v8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:30:23.133578Z",
     "iopub.status.busy": "2025-11-14T00:30:23.133263Z",
     "iopub.status.idle": "2025-11-14T00:30:24.121957Z",
     "shell.execute_reply": "2025-11-14T00:30:24.121398Z",
     "shell.execute_reply.started": "2025-11-14T00:30:23.133543Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Same Code but since we used DDP earlier, we have to initialise everything Again\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Config\n",
    "CONTEXT_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "HIDDEN_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 5\n",
    "DROPOUT = 0.1\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "class ChunkedSequenceDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, hf_dataset, word2idx, context_len, sos_id, eos_id, unk_id):\n",
    "        self.word2idx = word2idx\n",
    "        self.context_len = context_len\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.unk_id = unk_id\n",
    "\n",
    "        print(f\"Preparing sequences (context_len={context_len})...\")\n",
    "        self.chunks = self._prepare_chunks(hf_dataset)\n",
    "        print(f\"Created {len(self.chunks):,} chunks total.\")\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        return [self.sos_id] + [self.word2idx.get(tok, self.unk_id) for tok in tokens] + [self.eos_id]\n",
    "\n",
    "    def _prepare_chunks(self, dataset):\n",
    "        all_chunks = []\n",
    "        for item in tqdm(dataset, desc=\"Converting to chunks\"):\n",
    "            tokens = item.get(\"tokens\", None)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            ids = self._tokens_to_ids(tokens)\n",
    "\n",
    "            # Chunk into (context_len + 1) for input/target shifting \n",
    "            for i in range(0, max(1, len(ids) - 1), self.context_len + 1):\n",
    "                chunk = ids[i : i + self.context_len + 1]  \n",
    "                if len(chunk) > 1:  \n",
    "                    all_chunks.append(chunk)\n",
    "        return all_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each chunk is already a list[int]\n",
    "        return torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Collate Function ‚Äî Handles padding + input/target split\n",
    "def collate_batch(batch, pad_id, context_len):\n",
    "    \n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        input_seq = seq[:-1][:context_len]   # up to context_len (leftmost tokens)\n",
    "        target_seq = seq[1:][:context_len]   # shifted by one done, will not do in training now\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "\n",
    "    padded_inputs = pad_sequence(input_seqs, batch_first=True, padding_value=pad_id)\n",
    "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "    if padded_inputs.size(1) < context_len:\n",
    "        pad_width = context_len - padded_inputs.size(1)\n",
    "        pad_tensor = torch.full((padded_inputs.size(0), pad_width), pad_id, dtype=torch.long)\n",
    "        padded_inputs = torch.cat([padded_inputs, pad_tensor], dim=1)\n",
    "        padded_targets = torch.cat([padded_targets, pad_tensor], dim=1)\n",
    "    elif padded_inputs.size(1) > context_len:\n",
    "        padded_inputs = padded_inputs[:, :context_len]\n",
    "        padded_targets = padded_targets[:, :context_len]\n",
    "\n",
    "    # Attention mask: 1 = real token, 0 = pad\n",
    "    attention_mask = (padded_inputs != pad_id).long()\n",
    "\n",
    "    return padded_inputs, padded_targets, attention_mask\n",
    "\n",
    "\n",
    "# Layers \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model):\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.from_numpy(pe)  # (max_len, d_model)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, output_attentions: bool = False):\n",
    "        # x: (b, t, d_model)\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        q = self.wq(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scores: (b, heads, t, t)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # masking + softmax in float32 to avoid fp16 overflow\n",
    "        orig_dtype = scores.dtype\n",
    "        scores = scores.float()\n",
    "\n",
    "        causal = torch.tril(torch.ones((t, t), dtype=torch.bool, device=x.device))\n",
    "        allowed = causal.unsqueeze(0).unsqueeze(0)  # (1,1,t,t)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            pad_bool = padding_mask if padding_mask.dtype == torch.bool else (padding_mask == 0)\n",
    "            key_is_real = (~pad_bool).unsqueeze(1).unsqueeze(2)  # (b,1,1,t)\n",
    "            allowed = allowed & key_is_real\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "        else:\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "\n",
    "        scores = scores.masked_fill(~allowed, torch.finfo(scores.dtype).min)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        attn = attn.to(v.dtype)\n",
    "        context = torch.matmul(attn, v)  # (b, heads, t, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(b, t, self.d_model)\n",
    "        out = self.wo(context)\n",
    "\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or (4 * d_model)\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.activation(self.fc1(x))))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, hidden_dim=4 * d_model, dropout=dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None,output_attentions=False):\n",
    "        # Self-attention (with pre-LN)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, attn_weights = self.self_attn(x_norm, padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm2 = self.ln2(x)\n",
    "        ff_out = self.ff(x_norm2)\n",
    "        x = x + self.dropout(ff_out)\n",
    "\n",
    "        return x,attn_weights\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, max_len, dropout=0.1, embedding_weights=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embedding.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "            if freeze_embeddings:\n",
    "                self.token_embedding.weight.requires_grad = False\n",
    "\n",
    "        pe = sinusoidal_positional_encoding(max_len, d_model)  # (max_len, d_model)\n",
    "        self.register_buffer(\"position_encoding\", pe)  # persistent buffer\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.final_ln = LayerNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,output_attentions=False):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        b, t = input_ids.size()\n",
    "        tok_emb = self.token_embedding(input_ids)  # (b, t, d_model)\n",
    "        pos_emb = self.position_encoding[:t, :].unsqueeze(0).expand(b, -1, -1)  # (b, t, d_model)\n",
    "        x = tok_emb + pos_emb  # (b, t, d_model)\n",
    "\n",
    "        # attention_mask: (b, t) with 1 for real tokens, 0 for pad\n",
    "        if attention_mask is None:\n",
    "            key_padding_mask = None\n",
    "        else:\n",
    "            \n",
    "            key_padding_mask = (attention_mask == 0)  # shape (b, t), dtype=bool\n",
    "\n",
    "\n",
    "        all_attentions = []\n",
    "  \n",
    "        for layer in self.layers:\n",
    "            if output_attentions:\n",
    "                x, layer_attn = layer(x, key_padding_mask=key_padding_mask, output_attentions=True)\n",
    "                all_attentions.append(layer_attn)  # list of tensors (b, heads, t, t)\n",
    "            else:\n",
    "                x,_ = layer(x, key_padding_mask=key_padding_mask)\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.output_linear(x)\n",
    "        return logits, all_attentions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:30:24.123034Z",
     "iopub.status.busy": "2025-11-14T00:30:24.122777Z",
     "iopub.status.idle": "2025-11-14T00:30:25.044216Z",
     "shell.execute_reply": "2025-11-14T00:30:25.043545Z",
     "shell.execute_reply.started": "2025-11-14T00:30:24.123013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# === Load vocabulary ===\n",
    "with open(\"/kaggle/input/300dim-utils/vocab_300dim.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "word2idx = vocab_data['word2idx']\n",
    "idx2word = vocab_data['idx2word']\n",
    "\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "print(f\"Loaded vocab of size {VOCAB_SIZE}\")\n",
    "\n",
    "# === Load embedding matrix ===\n",
    "with open(\"/kaggle/input/300dim-utils/embedding_matrix_300dim.pkl\", \"rb\") as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "print(f\"Loaded embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# === Initialize model ===\n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    max_len=CONTEXT_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_weights=embedding_matrix,\n",
    "    freeze_embeddings=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# === Load checkpoint (non-DDP) ===\n",
    "ckpt = torch.load(\"/kaggle/working/checkpoints/epoch_10.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\" Loaded model from epoch {ckpt['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-14T00:31:05.210498Z",
     "iopub.status.busy": "2025-11-14T00:31:05.210210Z",
     "iopub.status.idle": "2025-11-14T00:31:13.667096Z",
     "shell.execute_reply": "2025-11-14T00:31:13.666402Z",
     "shell.execute_reply.started": "2025-11-14T00:31:05.210479Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: pyarrow, evaluate\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:31:38.210264Z",
     "iopub.status.busy": "2025-11-14T00:31:38.209932Z",
     "iopub.status.idle": "2025-11-14T00:32:07.766318Z",
     "shell.execute_reply": "2025-11-14T00:32:07.765535Z",
     "shell.execute_reply.started": "2025-11-14T00:31:38.210233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 00:31:45.908040: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763080306.330322      87 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763080306.441561      87 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate version: 0.4.6\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "print(\"Evaluate version:\", evaluate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:57:55.400209Z",
     "iopub.status.busy": "2025-11-14T00:57:55.399910Z",
     "iopub.status.idle": "2025-11-14T00:57:56.541802Z",
     "shell.execute_reply": "2025-11-14T00:57:56.541052Z",
     "shell.execute_reply.started": "2025-11-14T00:57:55.400187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate  # you already used evaluate.load(\"bleu\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt_text, tokenizer, word2idx, idx2word,\n",
    "             max_new_tokens=50, temperature=1.0, top_k=50, eos_token=\"[EOS]\",\n",
    "             max_total_len=64):\n",
    "    \"\"\"\n",
    "    Auto-regressive generation for DecoderOnlyTransformer.\n",
    "    Returns: (all_ids, generated_ids)\n",
    "      - all_ids: list of token ids including prompt and generated tokens\n",
    "      - generated_ids: list of generated ids (NOT including prompt ids)\n",
    "    NOTE: Works in id-space and does NOT re-tokenize generated output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokenize prompt using spaCy (or other tokenizer you used during vocab creation)\n",
    "    tokens = [t.text for t in tokenizer(prompt_text) if not t.is_space]\n",
    "    prompt_ids = [word2idx.get(tok, word2idx.get(\"[UNK]\", 0)) for tok in tokens]\n",
    "    if len(prompt_ids) == 0:\n",
    "        # avoid empty prompt\n",
    "        prompt_ids = [word2idx.get(\"[SOS]\", 0)] if \"[SOS]\" in word2idx else [0]\n",
    "\n",
    "    all_ids = prompt_ids.copy()\n",
    "    generated_ids = []\n",
    "\n",
    "    vocab_size = len(idx2word)\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        if len(all_ids) >= max_total_len:\n",
    "            \n",
    "            print(f\"Reached max_total_len={max_total_len}. Stopping generation.\")\n",
    "            break\n",
    "        # optional: enforce maximum total length (position encoding limit)\n",
    "        \n",
    "\n",
    "        input_ids = torch.tensor([all_ids], dtype=torch.long, device=device)  # (1, seq_len)\n",
    "        attention_mask = (input_ids != word2idx.get(\"[PAD]\", 0)).long()  # 1: real, 0: pad\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask=attention_mask)  # (1, seq_len, vocab)\n",
    "        next_token_logits = logits[:, -1, :]  # (1, vocab)\n",
    "        # apply temperature\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / float(temperature)\n",
    "\n",
    "        # top-k sampling\n",
    "        k = min(int(top_k), next_token_logits.size(-1))\n",
    "        topk_vals, topk_idx = torch.topk(next_token_logits, k=k, dim=-1)  # shapes (1,k)\n",
    "        probs = F.softmax(topk_vals, dim=-1)  # (1, k)\n",
    "        # sample (works for CPU/GPU)\n",
    "        sampled_idx_in_topk = torch.multinomial(probs[0], num_samples=1).item()  # scalar 0..k-1\n",
    "        next_token = int(topk_idx[0, sampled_idx_in_topk].item())  # the actual vocab id\n",
    "\n",
    "        all_ids.append(next_token)\n",
    "        generated_ids.append(next_token)\n",
    "\n",
    "        # stop on EOS\n",
    "        if idx2word.get(next_token, \"\") == eos_token:\n",
    "            break\n",
    "            \n",
    "    all_tokens = [idx2word.get(i, \"[UNK]\") for i in all_ids]\n",
    "    gen_tokens = [idx2word.get(i, \"[UNK]\") for i in generated_ids]\n",
    "    return gen_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# ---------- Beam search generator ----------\n",
    "@torch.no_grad()\n",
    "def beam_search_generate(\n",
    "        model, prompt_text, tokenizer, word2idx, idx2word,\n",
    "        beam_size=5, max_new_tokens=50, eos_token=\"[EOS]\",\n",
    "        max_total_len=64, length_penalty=1.0):\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    \n",
    "    tokens = [t.text for t in tokenizer(prompt_text) if not t.is_space]\n",
    "    prompt_ids = [word2idx.get(tok, word2idx.get(\"[UNK]\", 0)) for tok in tokens]\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [word2idx.get(\"[SOS]\", 0)]\n",
    "\n",
    "    eos_id = word2idx.get(eos_token, None)\n",
    "    pad_id = word2idx.get(\"[PAD]\", 0)\n",
    "\n",
    "    vocab_size = len(idx2word)\n",
    "\n",
    "    # Beam = (token_ids, raw_logprob_sum, finished_flag)\n",
    "    beams = [(prompt_ids.copy(), 0.0, False)]\n",
    "\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "\n",
    "        # stop if all beams finished\n",
    "        if all(b[2] for b in beams):\n",
    "            break\n",
    "\n",
    "        candidates = []\n",
    "\n",
    "        for ids, score, finished in beams:\n",
    "\n",
    "            # Keep finished beams unchanged\n",
    "            if finished or len(ids) >= max_total_len:\n",
    "                candidates.append((ids, score, True))\n",
    "                continue\n",
    "\n",
    "            input_ids = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "            attn_mask = (input_ids != pad_id).long()\n",
    "\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask)\n",
    "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)[0]\n",
    "\n",
    "            topk = min(beam_size, vocab_size)\n",
    "            vals, idxs = torch.topk(log_probs, k=topk)\n",
    "\n",
    "            for v, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "                new_ids = ids + [idx]\n",
    "                new_score = score + v\n",
    "                done = (idx == eos_id)\n",
    "                candidates.append((new_ids, new_score, done))\n",
    "\n",
    "        \n",
    "        scored = []\n",
    "        for ids, s, f in candidates:\n",
    "            lp = (len(ids) ** length_penalty)\n",
    "            lp = max(lp, 1e-6)\n",
    "            norm = s / lp\n",
    "            scored.append((norm, s, ids, f))\n",
    "\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        scored = scored[:beam_size]\n",
    "\n",
    "        beams = [(ids, raw, flag) for (norm, raw, ids, flag) in scored]\n",
    "\n",
    "    \n",
    "    finished = [b for b in beams if b[2]]\n",
    "    if len(finished) > 0:\n",
    "        best_ids, best_score, _ = max(finished, key=lambda x: x[1])\n",
    "    else:\n",
    "        best_ids, best_score, _ = max(beams, key=lambda x: x[1])\n",
    "\n",
    "    \n",
    "    gen_ids = best_ids[len(prompt_ids):]\n",
    "    gen_tokens = [idx2word.get(i, \"[UNK]\") for i in gen_ids]\n",
    "    gen_tokens = [t for t in gen_tokens if t not in (\"[SOS]\", \"[PAD]\", \"[EOS]\")]\n",
    "\n",
    "    return gen_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, there was an island\"\n",
    "out = beam_search_generate(\n",
    "    model, prompt, tokenizer, word2idx, idx2word,\n",
    "    beam_size=5, max_new_tokens=40\n",
    ")\n",
    "print(\"Generated:\", \" \".join(out))\n",
    "generated_text = generate(\n",
    "        model=model,\n",
    "        prompt_text=prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        word2idx=word2idx,\n",
    "        idx2word=idx2word,\n",
    "        max_new_tokens=40,\n",
    "        temperature=0.90,\n",
    "        top_k=50,\n",
    "    )\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {' '.join(generated_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8689231,
     "sourceId": 13666463,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
