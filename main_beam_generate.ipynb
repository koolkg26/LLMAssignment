{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:29.730850Z",
     "iopub.status.busy": "2025-11-14T00:13:29.730592Z",
     "iopub.status.idle": "2025-11-14T00:13:30.049078Z",
     "shell.execute_reply": "2025-11-14T00:13:30.048268Z",
     "shell.execute_reply.started": "2025-11-14T00:13:29.730831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:30.050399Z",
     "iopub.status.busy": "2025-11-14T00:13:30.049918Z",
     "iopub.status.idle": "2025-11-14T00:13:49.425633Z",
     "shell.execute_reply": "2025-11-14T00:13:49.425032Z",
     "shell.execute_reply.started": "2025-11-14T00:13:30.050373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:49.955278Z",
     "iopub.status.busy": "2025-11-14T00:13:49.955019Z",
     "iopub.status.idle": "2025-11-14T00:13:54.963551Z",
     "shell.execute_reply": "2025-11-14T00:13:54.962376Z",
     "shell.execute_reply.started": "2025-11-14T00:13:49.955250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:54.968949Z",
     "iopub.status.busy": "2025-11-14T00:13:54.967897Z",
     "iopub.status.idle": "2025-11-14T00:13:56.102940Z",
     "shell.execute_reply": "2025-11-14T00:13:56.102307Z",
     "shell.execute_reply.started": "2025-11-14T00:13:54.968908Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "\n",
    "def clean_and_normalize_text(text: str) -> str:\n",
    "    # Step 1: Use ftfy for general mojibake fixing\n",
    "    text = fix_text(text)\n",
    "\n",
    "    # Step 2: Replace known leftover mojibake manually\n",
    "    replacements = {\n",
    "        # Common double quotes (mojibake)\n",
    "        'â€œ': '\"',  # “\n",
    "        'â€': '\"',  # ” (some datasets use this variant)\n",
    "        'â€': '\"',   # leftover quote fragments\n",
    "        'Â«': '\"',   # «\n",
    "        'Â»': '\"',   # »\n",
    "    \n",
    "        # Common single quotes / apostrophes\n",
    "        'â€˜': \"'\",  # ‘\n",
    "        'â€™': \"'\",  # ’\n",
    "        'â€²': \"'\",  # ′ (prime used instead of apostrophe sometimes)\n",
    "        'â€³': '\"',  # ″ double prime → double quote\n",
    "    \n",
    "        # En dash / em dash\n",
    "        'â€“': '-',  # –\n",
    "        'â€”': '-',  # —\n",
    "        'âˆ’': '-',  # − (minus symbol)\n",
    "    \n",
    "        # Ellipsis\n",
    "        'â€¦': '...',  # …\n",
    "    \n",
    "        # Currency symbols (guess replacements)\n",
    "        'â‚¬': '€',  # euro\n",
    "        'â‚£': '£',  # pound\n",
    "        'â‚¥': '¥',  # yen\n",
    "    \n",
    "        # Bullets / middots\n",
    "        'â€¢': '•',  # bullet\n",
    "        'Â·': '·',   # middle dot\n",
    "    \n",
    "        # Accented letter fixes (when partial ftfy fails)\n",
    "        'Ã©': 'é',\n",
    "        'Ã¨': 'è',\n",
    "        'Ã¢': 'â',\n",
    "        'Ã´': 'ô',\n",
    "        'Ãà': 'à',\n",
    "        'Ãª': 'ê',\n",
    "        'Ã«': 'ë',\n",
    "        'Ã¹': 'ù',\n",
    "        'Ã¼': 'ü',\n",
    "        'Ã¶': 'ö',\n",
    "        'Ã„': 'Ä',\n",
    "        'Ãœ': 'Ü',\n",
    "        'Ã–': 'Ö',\n",
    "        'ÃŸ': 'ß',\n",
    "        'Ã±': 'ñ',\n",
    "    \n",
    "        # Occasionally seen garbage characters\n",
    "        'Â': '',     # stray non-breaking space marker\n",
    "        'âˆ†': '',   # delta artifact\n",
    "        'âˆž': '∞',  # infinity sign\n",
    "        'â„¢': '™',  # trademark\n",
    "        'âš«': '',   # stray symbol\n",
    "        'âœ”': '✔',  # checkmark\n",
    "    \n",
    "        # Daggers and unknown artifacts\n",
    "        '†': ' ',\n",
    "        '‡': ' ',\n",
    "        '�': ''      # replacement char for unknown glyph\n",
    "    }\n",
    "\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "\n",
    "    # # Step 3: Normalize repeated/mismatched quotes (optional formatting cleanup)\n",
    "    # text = re.sub(r'\\s+\"', '\"', text)          # Remove extra leading spaces before quotes\n",
    "    # text = re.sub(r'\"\\s+', '\" ', text)         # Ensure a space after closing quotes when needed\n",
    "\n",
    "    # # Step 4: Compact multiple spaces\n",
    "    # text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # # Step 5: Strip leading/trailing spaces\n",
    "    # text = text.strip()\n",
    "\n",
    "    return text\n",
    "for i, item in enumerate(ds[\"train\"]):\n",
    "    if i == 529932:\n",
    "        print(\"j\")\n",
    "        text = clean_and_normalize_text(item['text'])  # Apply ftfy here\n",
    "        \n",
    "        if \"Once upon a time there was a small, humble dog named Mittens. She loved to go for walks in the woods,\" in text:\n",
    "            print(\"Found at index:\", i)\n",
    "            print(text)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:56.104489Z",
     "iopub.status.busy": "2025-11-14T00:13:56.103812Z",
     "iopub.status.idle": "2025-11-14T00:13:58.276720Z",
     "shell.execute_reply": "2025-11-14T00:13:58.275811Z",
     "shell.execute_reply.started": "2025-11-14T00:13:56.104463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:13:58.278012Z",
     "iopub.status.busy": "2025-11-14T00:13:58.277539Z",
     "iopub.status.idle": "2025-11-14T00:14:14.862696Z",
     "shell.execute_reply": "2025-11-14T00:14:14.861857Z",
     "shell.execute_reply.started": "2025-11-14T00:13:58.277986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading spaCy model 'en_core_web_sm'...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "tokenizer = nlp.tokenizer\n",
    "print(\"spaCy tokenizer ready.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes a single text string.\"\"\"\n",
    "    tokens = [t.text for t in tokenizer(str(text)) if not t.is_space]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_ds_path = \"./tokenized_hf_dataset\"\n",
    "final_combined_path = \"./combined_tokenized_texts_300dim_v2.pkl\"\n",
    "\n",
    "print(\"\\n Tokenization Begins ...\")\n",
    "\n",
    "def process_batch(batch):\n",
    "    # THE FIX: Apply the cleaning function before tokenizing\n",
    "    cleaned_texts = [clean_and_normalize_text(t) for t in batch[\"text\"]]\n",
    "    return {\"tokens\": [tokenize_text(t) for t in cleaned_texts]}\n",
    "\n",
    "for split in [\"train\", \"validation\"]:\n",
    "    if split not in ds:\n",
    "        print(f\"⚠️ Split '{split}' not found in dataset — skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing '{split}' split...\")\n",
    "    num_proc = min(4, os.cpu_count() or 1)\n",
    "\n",
    "    tokenized_split = ds[split].map(\n",
    "        process_batch,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=num_proc,\n",
    "        desc=f\"Cleaning & Tokenizing {split}\"\n",
    "    )\n",
    "    split_save_path = os.path.join(tokenized_ds_path, split)\n",
    "    tokenized_split.save_to_disk(split_save_path)\n",
    "    print(f\"Finished and saved '{split}' to '{split_save_path}'.\")\n",
    "\n",
    "# Commented section was used to make vocabulary from train and val both\n",
    "\n",
    "# if os.path.exists(final_combined_path):\n",
    "#     print(f\"\\n Found final combined file at '{final_combined_path}'. Nothing to do.\")\n",
    "# else:\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"\\n Streaming tokenized splits to a single .pkl file: '{final_combined_path}'...\")\n",
    "    \n",
    "#     total_lines = 0\n",
    "#     with open(final_combined_path, \"wb\") as f_out:\n",
    "#         for split in [\"train\", \"validation\"]:\n",
    "#             split_save_path = os.path.join(tokenized_ds_path, split)\n",
    "#             if not os.path.exists(split_save_path):\n",
    "#                 continue\n",
    "            \n",
    "#             print(f\"  Streaming from '{split}' split...\")\n",
    "#             tokenized_dataset = Dataset.load_from_disk(split_save_path)\n",
    "\n",
    "#             for example in tqdm(tokenized_dataset, desc=f\"  Pickling {split}\"):\n",
    "#                 tokens = example['tokens']\n",
    "#                 if tokens:\n",
    "#                     pickle.dump(tokens, f_out)\n",
    "#                     total_lines += 1\n",
    "\n",
    "#     # Save metadata for the vocab builder's progress bar\n",
    "#     # with open(metadata_path, \"w\") as f_meta:\n",
    "#     #     json.dump({\"total_lines\": total_lines}, f_meta)\n",
    "\n",
    "#     print(\"\\n Streaming complete.\")\n",
    "#     print(f\"Final combined file '{final_combined_path}' created with {total_lines:,} tokenized sentences.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.200538Z",
     "iopub.status.busy": "2025-11-14T00:14:15.200380Z",
     "iopub.status.idle": "2025-11-14T00:14:15.219511Z",
     "shell.execute_reply": "2025-11-14T00:14:15.218954Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.200525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.220739Z",
     "iopub.status.busy": "2025-11-14T00:14:15.220516Z",
     "iopub.status.idle": "2025-11-14T00:14:15.242448Z",
     "shell.execute_reply": "2025-11-14T00:14:15.241863Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.220725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train_ddp_v8.py\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Config\n",
    "\n",
    "CONTEXT_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "HIDDEN_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 5\n",
    "DROPOUT = 0.1\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "# Dataset \n",
    "class ChunkedSequenceDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, hf_dataset, word2idx, context_len, sos_id, eos_id, unk_id):\n",
    "        self.word2idx = word2idx\n",
    "        self.context_len = context_len\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.unk_id = unk_id\n",
    "\n",
    "        print(f\"Preparing sequences (context_len={context_len})...\")\n",
    "        self.chunks = self._prepare_chunks(hf_dataset)\n",
    "        print(f\"Created {len(self.chunks):,} chunks total.\")\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        return [self.sos_id] + [self.word2idx.get(tok, self.unk_id) for tok in tokens] + [self.eos_id]\n",
    "\n",
    "    def _prepare_chunks(self, dataset):\n",
    "        all_chunks = []\n",
    "        for item in tqdm(dataset, desc=\"Converting to chunks\"):\n",
    "            tokens = item.get(\"tokens\", None)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            ids = self._tokens_to_ids(tokens)\n",
    "\n",
    "            # Chunk into (context_len + 1) for input/target shifting \n",
    "            for i in range(0, max(1, len(ids) - 1), self.context_len + 1):\n",
    "                chunk = ids[i : i + self.context_len + 1]  \n",
    "                if len(chunk) > 1:  \n",
    "                    all_chunks.append(chunk)\n",
    "        return all_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each chunk is already a list[int]\n",
    "        return torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Collate Function — Handles padding + input/target split\n",
    "def collate_batch(batch, pad_id, context_len):\n",
    "    \n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        input_seq = seq[:-1][:context_len]   # up to context_len (leftmost tokens)\n",
    "        target_seq = seq[1:][:context_len]   # shifted by one done, will not do in training now\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "\n",
    "    padded_inputs = pad_sequence(input_seqs, batch_first=True, padding_value=pad_id)\n",
    "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "    if padded_inputs.size(1) < context_len:\n",
    "        pad_width = context_len - padded_inputs.size(1)\n",
    "        pad_tensor = torch.full((padded_inputs.size(0), pad_width), pad_id, dtype=torch.long)\n",
    "        padded_inputs = torch.cat([padded_inputs, pad_tensor], dim=1)\n",
    "        padded_targets = torch.cat([padded_targets, pad_tensor], dim=1)\n",
    "    elif padded_inputs.size(1) > context_len:\n",
    "        padded_inputs = padded_inputs[:, :context_len]\n",
    "        padded_targets = padded_targets[:, :context_len]\n",
    "\n",
    "    # Attention mask: 1 = real token, 0 = pad\n",
    "    attention_mask = (padded_inputs != pad_id).long()\n",
    "\n",
    "    return padded_inputs, padded_targets, attention_mask\n",
    "\n",
    "\n",
    "# Layers \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model):\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.from_numpy(pe)  # (max_len, d_model)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, output_attentions: bool = False):\n",
    "        # x: (b, t, d_model)\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        q = self.wq(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scores: (b, heads, t, t)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # masking + softmax in float32 to avoid fp16 overflow\n",
    "        orig_dtype = scores.dtype\n",
    "        scores = scores.float()\n",
    "\n",
    "        causal = torch.tril(torch.ones((t, t), dtype=torch.bool, device=x.device))\n",
    "        allowed = causal.unsqueeze(0).unsqueeze(0)  # (1,1,t,t)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            pad_bool = padding_mask if padding_mask.dtype == torch.bool else (padding_mask == 0)\n",
    "            key_is_real = (~pad_bool).unsqueeze(1).unsqueeze(2)  # (b,1,1,t)\n",
    "            allowed = allowed & key_is_real\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "        else:\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "\n",
    "        scores = scores.masked_fill(~allowed, torch.finfo(scores.dtype).min)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        attn = attn.to(v.dtype)\n",
    "        context = torch.matmul(attn, v)  # (b, heads, t, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(b, t, self.d_model)\n",
    "        out = self.wo(context)\n",
    "\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or (4 * d_model)\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.activation(self.fc1(x))))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, hidden_dim=4 * d_model, dropout=dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None,output_attentions=False):\n",
    "        # Self-attention (with pre-LN)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, attn_weights = self.self_attn(x_norm, padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm2 = self.ln2(x)\n",
    "        ff_out = self.ff(x_norm2)\n",
    "        x = x + self.dropout(ff_out)\n",
    "\n",
    "        return x,attn_weights\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, max_len, dropout=0.1, embedding_weights=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embedding.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "            if freeze_embeddings:\n",
    "                self.token_embedding.weight.requires_grad = False\n",
    "\n",
    "        pe = sinusoidal_positional_encoding(max_len, d_model)  # (max_len, d_model)\n",
    "        self.register_buffer(\"position_encoding\", pe)  # persistent buffer\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.final_ln = LayerNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,output_attentions=False):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        b, t = input_ids.size()\n",
    "        tok_emb = self.token_embedding(input_ids)  # (b, t, d_model)\n",
    "        pos_emb = self.position_encoding[:t, :].unsqueeze(0).expand(b, -1, -1)  # (b, t, d_model)\n",
    "        x = tok_emb + pos_emb  # (b, t, d_model)\n",
    "\n",
    "        # attention_mask: (b, t) with 1 for real tokens, 0 for pad\n",
    "        if attention_mask is None:\n",
    "            key_padding_mask = None\n",
    "        else:\n",
    "            \n",
    "            key_padding_mask = (attention_mask == 0)  # shape (b, t), dtype=bool\n",
    "\n",
    "\n",
    "        all_attentions = []\n",
    "  \n",
    "        for layer in self.layers:\n",
    "            if output_attentions:\n",
    "                x, layer_attn = layer(x, key_padding_mask=key_padding_mask, output_attentions=True)\n",
    "                all_attentions.append(layer_attn)  # list of tensors (b, heads, t, t)\n",
    "            else:\n",
    "                x,_ = layer(x, key_padding_mask=key_padding_mask)\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.output_linear(x)\n",
    "        return logits, all_attentions\n",
    "\n",
    "\n",
    "\n",
    "# Training / Main\n",
    "def main():\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Device:', DEVICE)\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "\n",
    "    tokenized_ds_path = \"tokenized_hf_dataset\"\n",
    "    VOCAB_SAVE_PATH = \"/kaggle/input/300dim-utils/vocab_300dim.pkl\"\n",
    "    EMBEDDING_MATRIX_SAVE_PATH = \"/kaggle/input/300dim-utils/embedding_matrix_300dim.pkl\"\n",
    "\n",
    "    # Load vocab\n",
    "    print(f\"\\n Found saved vocabulary file at '{VOCAB_SAVE_PATH}'. Loading...\")\n",
    "    with open(VOCAB_SAVE_PATH, \"rb\") as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "        word2idx = vocab_data['word2idx']\n",
    "        idx2word = vocab_data['idx2word']\n",
    "    print(f\"Loaded vocabulary with {len(word2idx):,} tokens.\")\n",
    "    VOCAB_SIZE = len(word2idx)\n",
    "\n",
    "    # Load embedding matrix\n",
    "    EMBEDDING_DIM = 100\n",
    "    print(f\"\\n Found saved embedding matrix file at '{EMBEDDING_MATRIX_SAVE_PATH}'. Loading...\")\n",
    "    with open(EMBEDDING_MATRIX_SAVE_PATH, \"rb\") as f:\n",
    "        embedding_matrix = pickle.load(f)\n",
    "    print(f\"Loaded embedding matrix with shape {embedding_matrix.shape}.\")\n",
    "\n",
    "    print(\" Loading tokenized dataset...\")\n",
    "    from datasets import load_from_disk\n",
    "    train_ds = load_from_disk(os.path.join(tokenized_ds_path, \"train\"))\n",
    "    val_ds = load_from_disk(os.path.join(tokenized_ds_path, \"validation\"))\n",
    "\n",
    "    # Build PyTorch Datasets\n",
    "    print(\"Building Dataset\")\n",
    "    train_dataset = ChunkedSequenceDataset(train_ds, word2idx, CONTEXT_LEN, SOS_ID, EOS_ID, UNK_ID)\n",
    "    val_dataset = ChunkedSequenceDataset(val_ds, word2idx, CONTEXT_LEN, SOS_ID, EOS_ID, UNK_ID)\n",
    "\n",
    "    # DataLoaders\n",
    "    collate_fn = partial(collate_batch, pad_id=PAD_ID, context_len=CONTEXT_LEN)\n",
    "\n",
    "    print(\"Building Sampler\")\n",
    "    train_sampler = DistributedSampler(train_dataset, shuffle=True)\n",
    "    val_sampler = DistributedSampler(val_dataset, shuffle=False)\n",
    "\n",
    "    print(\"Building Loader\")\n",
    "    SAVE_DIR = \"./checkpoints\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    USE_WANDB = True\n",
    "\n",
    "    # Init wandb only on rank 0\n",
    "    rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "\n",
    "    os.environ[\"WANDB_MODE\"] = \"offline\"  s\n",
    "    \n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.require(\"service\")\n",
    "        wandb.init(\n",
    "            project=\"transformer-training\",\n",
    "            mode=\"offline\",   \n",
    "            config={\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"lr\": LR,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"hidden_dim\": HIDDEN_DIM,\n",
    "                \"num_layers\": NUM_LAYERS,\n",
    "                \"num_heads\": NUM_HEADS\n",
    "            }\n",
    "        )\n",
    "   \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              sampler=train_sampler, collate_fn=collate_fn,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            sampler=val_sampler, collate_fn=collate_fn,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    print(\"Starting Model\")\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        max_len=CONTEXT_LEN,\n",
    "        dropout=DROPOUT,\n",
    "        embedding_weights=embedding_matrix,\n",
    "        freeze_embeddings=True\n",
    "    ).to(device)\n",
    "\n",
    "    model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "    pad_idx = PAD_ID\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    train_losses, val_losses, perplexities = [], [], []\n",
    "    \n",
    "    # =============== TRAINING LOOP ===============\n",
    "    print(\"Training Begins....\")\n",
    "    # Path to checkpoint\n",
    "    CKPT_PATH = \"./epoch_10.pt\"  # example: resume from epoch 5\n",
    "    START_EPOCH = 1  # default if no checkpoint\n",
    "    \n",
    "    if os.path.exists(CKPT_PATH):\n",
    "        print(f\"Loading checkpoint from {CKPT_PATH} ...\")\n",
    "        checkpoint = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "    \n",
    "        # Restore model parameters\n",
    "        model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "        train_losses = checkpoint.get(\"train_losses\", [])\n",
    "        val_losses = checkpoint.get(\"val_losses\", [])\n",
    "        perplexities = checkpoint.get(\"perplexities\", [])\n",
    "    \n",
    "        # Continue from  epoch\n",
    "        START_EPOCH = checkpoint[\"epoch\"] + 1\n",
    "    \n",
    "        print(f\" Resumed from epoch {checkpoint['epoch']} — continuing from epoch {START_EPOCH}\")\n",
    "    else:\n",
    "        print(\" No checkpoint found — starting training from scratch\")\n",
    "\n",
    "    \n",
    "    for epoch in range(START_EPOCH, EPOCHS + 1):\n",
    "        torch.cuda.reset_peak_memory_stats(DEVICE)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_steps = 0\n",
    "    \n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch} train\")\n",
    "    \n",
    "        for step, (input_ids, target_ids, _) in pbar:\n",
    "            input_ids = input_ids.to(DEVICE, non_blocking=True)\n",
    "            target_ids = target_ids.to(DEVICE, non_blocking=True)\n",
    "            attention_mask = (input_ids != pad_idx).long()\n",
    "    \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits,_ = model(input_ids, attention_mask=attention_mask)\n",
    "                logits_flat = logits.view(-1, VOCAB_SIZE)\n",
    "                targets_flat = target_ids.view(-1)\n",
    "                loss = criterion(logits_flat, targets_flat)\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            avg_loss = running_loss / total_steps\n",
    "            pbar.set_postfix({'train_loss': avg_loss})\n",
    "    \n",
    "        train_loss = running_loss / total_steps\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        # =============== VALIDATION ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, target_ids, _ in tqdm(val_loader, desc=f\"Epoch {epoch} val\"):\n",
    "                input_ids = input_ids.to(DEVICE, non_blocking=True)\n",
    "                target_ids = target_ids.to(DEVICE, non_blocking=True)\n",
    "                attention_mask = (input_ids != pad_idx).long()\n",
    "    \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits,all_attns = model(input_ids, attention_mask=attention_mask, output_attentions = True)\n",
    "                    logits_flat = logits.view(-1, VOCAB_SIZE)\n",
    "                    targets_flat = target_ids.view(-1)\n",
    "                    loss = criterion(logits_flat, targets_flat)\n",
    "    \n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "    \n",
    "        val_loss /= val_steps\n",
    "        val_losses.append(val_loss)\n",
    "        perplexity = math.exp(val_loss)\n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "        peak_mem = torch.cuda.max_memory_allocated(DEVICE) / (1024 ** 3)\n",
    "    \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, PPL={perplexity:.2f}, PeakMem={peak_mem:.2f} GB\")\n",
    "    \n",
    "        if USE_WANDB:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"perplexity\": perplexity,\n",
    "                \"peak_gpu_memory_gb\": peak_mem\n",
    "            })\n",
    "    \n",
    "        # =============== CHECKPOINTING ===============\n",
    "        ckpt_path = os.path.join(SAVE_DIR, f\"epoch_{epoch}.pt\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.module.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"perplexities\": perplexities\n",
    "        }, ckpt_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"loss_curve.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, EPOCHS + 1), perplexities, label=\"Perplexity\", color=\"purple\", marker='o')\n",
    "    plt.title(\"Validation Perplexity Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"perplexity_curve.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.finish()\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:14:15.243377Z",
     "iopub.status.busy": "2025-11-14T00:14:15.243027Z",
     "iopub.status.idle": "2025-11-14T00:30:23.131840Z",
     "shell.execute_reply": "2025-11-14T00:30:23.131084Z",
     "shell.execute_reply.started": "2025-11-14T00:14:15.243358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 train_ddp_v8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:30:23.133578Z",
     "iopub.status.busy": "2025-11-14T00:30:23.133263Z",
     "iopub.status.idle": "2025-11-14T00:30:24.121957Z",
     "shell.execute_reply": "2025-11-14T00:30:24.121398Z",
     "shell.execute_reply.started": "2025-11-14T00:30:23.133543Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Same Code but since we used DDP earlier, we have to initialise everything Again\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Config\n",
    "CONTEXT_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "SOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UNK_ID = 1\n",
    "PAD_ID = 0\n",
    "HIDDEN_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 5\n",
    "DROPOUT = 0.1\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "class ChunkedSequenceDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, hf_dataset, word2idx, context_len, sos_id, eos_id, unk_id):\n",
    "        self.word2idx = word2idx\n",
    "        self.context_len = context_len\n",
    "        self.sos_id = sos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.unk_id = unk_id\n",
    "\n",
    "        print(f\"Preparing sequences (context_len={context_len})...\")\n",
    "        self.chunks = self._prepare_chunks(hf_dataset)\n",
    "        print(f\"Created {len(self.chunks):,} chunks total.\")\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        return [self.sos_id] + [self.word2idx.get(tok, self.unk_id) for tok in tokens] + [self.eos_id]\n",
    "\n",
    "    def _prepare_chunks(self, dataset):\n",
    "        all_chunks = []\n",
    "        for item in tqdm(dataset, desc=\"Converting to chunks\"):\n",
    "            tokens = item.get(\"tokens\", None)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            ids = self._tokens_to_ids(tokens)\n",
    "\n",
    "            # Chunk into (context_len + 1) for input/target shifting \n",
    "            for i in range(0, max(1, len(ids) - 1), self.context_len + 1):\n",
    "                chunk = ids[i : i + self.context_len + 1]  \n",
    "                if len(chunk) > 1:  \n",
    "                    all_chunks.append(chunk)\n",
    "        return all_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each chunk is already a list[int]\n",
    "        return torch.tensor(self.chunks[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Collate Function — Handles padding + input/target split\n",
    "def collate_batch(batch, pad_id, context_len):\n",
    "    \n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for seq in batch:\n",
    "        input_seq = seq[:-1][:context_len]   # up to context_len (leftmost tokens)\n",
    "        target_seq = seq[1:][:context_len]   # shifted by one done, will not do in training now\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "\n",
    "    padded_inputs = pad_sequence(input_seqs, batch_first=True, padding_value=pad_id)\n",
    "    padded_targets = pad_sequence(target_seqs, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "    if padded_inputs.size(1) < context_len:\n",
    "        pad_width = context_len - padded_inputs.size(1)\n",
    "        pad_tensor = torch.full((padded_inputs.size(0), pad_width), pad_id, dtype=torch.long)\n",
    "        padded_inputs = torch.cat([padded_inputs, pad_tensor], dim=1)\n",
    "        padded_targets = torch.cat([padded_targets, pad_tensor], dim=1)\n",
    "    elif padded_inputs.size(1) > context_len:\n",
    "        padded_inputs = padded_inputs[:, :context_len]\n",
    "        padded_targets = padded_targets[:, :context_len]\n",
    "\n",
    "    # Attention mask: 1 = real token, 0 = pad\n",
    "    attention_mask = (padded_inputs != pad_id).long()\n",
    "\n",
    "    return padded_inputs, padded_targets, attention_mask\n",
    "\n",
    "\n",
    "# Layers \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.weight * x_norm + self.bias\n",
    "\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, d_model):\n",
    "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return torch.from_numpy(pe)  # (max_len, d_model)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, output_attentions: bool = False):\n",
    "        # x: (b, t, d_model)\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        q = self.wq(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.wk(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.wv(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scores: (b, heads, t, t)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # masking + softmax in float32 to avoid fp16 overflow\n",
    "        orig_dtype = scores.dtype\n",
    "        scores = scores.float()\n",
    "\n",
    "        causal = torch.tril(torch.ones((t, t), dtype=torch.bool, device=x.device))\n",
    "        allowed = causal.unsqueeze(0).unsqueeze(0)  # (1,1,t,t)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            pad_bool = padding_mask if padding_mask.dtype == torch.bool else (padding_mask == 0)\n",
    "            key_is_real = (~pad_bool).unsqueeze(1).unsqueeze(2)  # (b,1,1,t)\n",
    "            allowed = allowed & key_is_real\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "        else:\n",
    "            allowed = allowed.expand(b, self.num_heads, t, t)\n",
    "\n",
    "        scores = scores.masked_fill(~allowed, torch.finfo(scores.dtype).min)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        attn = attn.to(v.dtype)\n",
    "        context = torch.matmul(attn, v)  # (b, heads, t, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(b, t, self.d_model)\n",
    "        out = self.wo(context)\n",
    "\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or (4 * d_model)\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.activation(self.fc1(x))))\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        self.ln1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, hidden_dim=4 * d_model, dropout=dropout)\n",
    "        self.ln2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None,output_attentions=False):\n",
    "        # Self-attention (with pre-LN)\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, attn_weights = self.self_attn(x_norm, padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # Feed-forward\n",
    "        x_norm2 = self.ln2(x)\n",
    "        ff_out = self.ff(x_norm2)\n",
    "        x = x + self.dropout(ff_out)\n",
    "\n",
    "        return x,attn_weights\n",
    "\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, max_len, dropout=0.1, embedding_weights=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if embedding_weights is not None:\n",
    "            self.token_embedding.weight.data.copy_(torch.from_numpy(embedding_weights))\n",
    "            if freeze_embeddings:\n",
    "                self.token_embedding.weight.requires_grad = False\n",
    "\n",
    "        pe = sinusoidal_positional_encoding(max_len, d_model)  # (max_len, d_model)\n",
    "        self.register_buffer(\"position_encoding\", pe)  # persistent buffer\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.final_ln = LayerNorm(d_model)\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,output_attentions=False):\n",
    "        # input_ids: (batch, seq_len)\n",
    "        b, t = input_ids.size()\n",
    "        tok_emb = self.token_embedding(input_ids)  # (b, t, d_model)\n",
    "        pos_emb = self.position_encoding[:t, :].unsqueeze(0).expand(b, -1, -1)  # (b, t, d_model)\n",
    "        x = tok_emb + pos_emb  # (b, t, d_model)\n",
    "\n",
    "        # attention_mask: (b, t) with 1 for real tokens, 0 for pad\n",
    "        if attention_mask is None:\n",
    "            key_padding_mask = None\n",
    "        else:\n",
    "            \n",
    "            key_padding_mask = (attention_mask == 0)  # shape (b, t), dtype=bool\n",
    "\n",
    "\n",
    "        all_attentions = []\n",
    "  \n",
    "        for layer in self.layers:\n",
    "            if output_attentions:\n",
    "                x, layer_attn = layer(x, key_padding_mask=key_padding_mask, output_attentions=True)\n",
    "                all_attentions.append(layer_attn)  # list of tensors (b, heads, t, t)\n",
    "            else:\n",
    "                x,_ = layer(x, key_padding_mask=key_padding_mask)\n",
    "        x = self.final_ln(x)\n",
    "        logits = self.output_linear(x)\n",
    "        return logits, all_attentions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:30:24.123034Z",
     "iopub.status.busy": "2025-11-14T00:30:24.122777Z",
     "iopub.status.idle": "2025-11-14T00:30:25.044216Z",
     "shell.execute_reply": "2025-11-14T00:30:25.043545Z",
     "shell.execute_reply.started": "2025-11-14T00:30:24.123013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# === Load vocabulary ===\n",
    "with open(\"/kaggle/input/300dim-utils/vocab_300dim.pkl\", \"rb\") as f:\n",
    "    vocab_data = pickle.load(f)\n",
    "word2idx = vocab_data['word2idx']\n",
    "idx2word = vocab_data['idx2word']\n",
    "\n",
    "VOCAB_SIZE = len(word2idx)\n",
    "print(f\"Loaded vocab of size {VOCAB_SIZE}\")\n",
    "\n",
    "# === Load embedding matrix ===\n",
    "with open(\"/kaggle/input/300dim-utils/embedding_matrix_300dim.pkl\", \"rb\") as f:\n",
    "    embedding_matrix = pickle.load(f)\n",
    "print(f\"Loaded embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# === Initialize model ===\n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    max_len=CONTEXT_LEN,\n",
    "    dropout=DROPOUT,\n",
    "    embedding_weights=embedding_matrix,\n",
    "    freeze_embeddings=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# === Load checkpoint (non-DDP) ===\n",
    "ckpt = torch.load(\"/kaggle/working/checkpoints/epoch_10.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(f\" Loaded model from epoch {ckpt['epoch']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-14T00:31:05.210498Z",
     "iopub.status.busy": "2025-11-14T00:31:05.210210Z",
     "iopub.status.idle": "2025-11-14T00:31:13.667096Z",
     "shell.execute_reply": "2025-11-14T00:31:13.666402Z",
     "shell.execute_reply.started": "2025-11-14T00:31:05.210479Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:31:38.210264Z",
     "iopub.status.busy": "2025-11-14T00:31:38.209932Z",
     "iopub.status.idle": "2025-11-14T00:32:07.766318Z",
     "shell.execute_reply": "2025-11-14T00:32:07.765535Z",
     "shell.execute_reply.started": "2025-11-14T00:31:38.210233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "print(\"Evaluate version:\", evaluate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T00:57:55.400209Z",
     "iopub.status.busy": "2025-11-14T00:57:55.399910Z",
     "iopub.status.idle": "2025-11-14T00:57:56.541802Z",
     "shell.execute_reply": "2025-11-14T00:57:56.541052Z",
     "shell.execute_reply.started": "2025-11-14T00:57:55.400187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import evaluate  # you already used evaluate.load(\"bleu\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt_text, tokenizer, word2idx, idx2word,\n",
    "             max_new_tokens=50, temperature=1.0, top_k=50, eos_token=\"[EOS]\",\n",
    "             max_total_len=64):\n",
    "    \"\"\"\n",
    "    Auto-regressive generation for DecoderOnlyTransformer.\n",
    "    Returns: (all_ids, generated_ids)\n",
    "      - all_ids: list of token ids including prompt and generated tokens\n",
    "      - generated_ids: list of generated ids (NOT including prompt ids)\n",
    "    NOTE: Works in id-space and does NOT re-tokenize generated output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Tokenize prompt using spaCy (or other tokenizer you used during vocab creation)\n",
    "    tokens = [t.text for t in tokenizer(prompt_text) if not t.is_space]\n",
    "    prompt_ids = [word2idx.get(tok, word2idx.get(\"[UNK]\", 0)) for tok in tokens]\n",
    "    if len(prompt_ids) == 0:\n",
    "        # avoid empty prompt\n",
    "        prompt_ids = [word2idx.get(\"[SOS]\", 0)] if \"[SOS]\" in word2idx else [0]\n",
    "\n",
    "    all_ids = prompt_ids.copy()\n",
    "    generated_ids = []\n",
    "\n",
    "    vocab_size = len(idx2word)\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        if len(all_ids) >= max_total_len:\n",
    "            \n",
    "            print(f\"Reached max_total_len={max_total_len}. Stopping generation.\")\n",
    "            break\n",
    "        # optional: enforce maximum total length (position encoding limit)\n",
    "        \n",
    "\n",
    "        input_ids = torch.tensor([all_ids], dtype=torch.long, device=device)  # (1, seq_len)\n",
    "        attention_mask = (input_ids != word2idx.get(\"[PAD]\", 0)).long()  # 1: real, 0: pad\n",
    "\n",
    "        logits, _ = model(input_ids, attention_mask=attention_mask)  # (1, seq_len, vocab)\n",
    "        next_token_logits = logits[:, -1, :]  # (1, vocab)\n",
    "        # apply temperature\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / float(temperature)\n",
    "\n",
    "        # top-k sampling\n",
    "        k = min(int(top_k), next_token_logits.size(-1))\n",
    "        topk_vals, topk_idx = torch.topk(next_token_logits, k=k, dim=-1)  # shapes (1,k)\n",
    "        probs = F.softmax(topk_vals, dim=-1)  # (1, k)\n",
    "        # sample (works for CPU/GPU)\n",
    "        sampled_idx_in_topk = torch.multinomial(probs[0], num_samples=1).item()  # scalar 0..k-1\n",
    "        next_token = int(topk_idx[0, sampled_idx_in_topk].item())  # the actual vocab id\n",
    "\n",
    "        all_ids.append(next_token)\n",
    "        generated_ids.append(next_token)\n",
    "\n",
    "        # stop on EOS\n",
    "        if idx2word.get(next_token, \"\") == eos_token:\n",
    "            break\n",
    "            \n",
    "    all_tokens = [idx2word.get(i, \"[UNK]\") for i in all_ids]\n",
    "    gen_tokens = [idx2word.get(i, \"[UNK]\") for i in generated_ids]\n",
    "    return gen_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# ---------- Beam search generator ----------\n",
    "@torch.no_grad()\n",
    "def beam_search_generate(\n",
    "        model, prompt_text, tokenizer, word2idx, idx2word,\n",
    "        beam_size=5, max_new_tokens=50, eos_token=\"[EOS]\",\n",
    "        max_total_len=64, length_penalty=1.0):\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    \n",
    "    tokens = [t.text for t in tokenizer(prompt_text) if not t.is_space]\n",
    "    prompt_ids = [word2idx.get(tok, word2idx.get(\"[UNK]\", 0)) for tok in tokens]\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [word2idx.get(\"[SOS]\", 0)]\n",
    "\n",
    "    eos_id = word2idx.get(eos_token, None)\n",
    "    pad_id = word2idx.get(\"[PAD]\", 0)\n",
    "\n",
    "    vocab_size = len(idx2word)\n",
    "\n",
    "    # Beam = (token_ids, raw_logprob_sum, finished_flag)\n",
    "    beams = [(prompt_ids.copy(), 0.0, False)]\n",
    "\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "\n",
    "        # stop if all beams finished\n",
    "        if all(b[2] for b in beams):\n",
    "            break\n",
    "\n",
    "        candidates = []\n",
    "\n",
    "        for ids, score, finished in beams:\n",
    "\n",
    "            # Keep finished beams unchanged\n",
    "            if finished or len(ids) >= max_total_len:\n",
    "                candidates.append((ids, score, True))\n",
    "                continue\n",
    "\n",
    "            input_ids = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "            attn_mask = (input_ids != pad_id).long()\n",
    "\n",
    "            logits, _ = model(input_ids, attention_mask=attn_mask)\n",
    "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)[0]\n",
    "\n",
    "            topk = min(beam_size, vocab_size)\n",
    "            vals, idxs = torch.topk(log_probs, k=topk)\n",
    "\n",
    "            for v, idx in zip(vals.tolist(), idxs.tolist()):\n",
    "                new_ids = ids + [idx]\n",
    "                new_score = score + v\n",
    "                done = (idx == eos_id)\n",
    "                candidates.append((new_ids, new_score, done))\n",
    "\n",
    "        \n",
    "        scored = []\n",
    "        for ids, s, f in candidates:\n",
    "            lp = (len(ids) ** length_penalty)\n",
    "            lp = max(lp, 1e-6)\n",
    "            norm = s / lp\n",
    "            scored.append((norm, s, ids, f))\n",
    "\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        scored = scored[:beam_size]\n",
    "\n",
    "        beams = [(ids, raw, flag) for (norm, raw, ids, flag) in scored]\n",
    "\n",
    "    \n",
    "    finished = [b for b in beams if b[2]]\n",
    "    if len(finished) > 0:\n",
    "        best_ids, best_score, _ = max(finished, key=lambda x: x[1])\n",
    "    else:\n",
    "        best_ids, best_score, _ = max(beams, key=lambda x: x[1])\n",
    "\n",
    "    \n",
    "    gen_ids = best_ids[len(prompt_ids):]\n",
    "    gen_tokens = [idx2word.get(i, \"[UNK]\") for i in gen_ids]\n",
    "    gen_tokens = [t for t in gen_tokens if t not in (\"[SOS]\", \"[PAD]\", \"[EOS]\")]\n",
    "\n",
    "    return gen_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, there was an island\"\n",
    "out = beam_search_generate(\n",
    "    model, prompt, tokenizer, word2idx, idx2word,\n",
    "    beam_size=5, max_new_tokens=40\n",
    ")\n",
    "print(\"Generated:\", \" \".join(out))\n",
    "generated_text = generate(\n",
    "        model=model,\n",
    "        prompt_text=prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        word2idx=word2idx,\n",
    "        idx2word=idx2word,\n",
    "        max_new_tokens=40,\n",
    "        temperature=0.90,\n",
    "        top_k=50,\n",
    "    )\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {' '.join(generated_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8689231,
     "sourceId": 13666463,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
